---
title: 'A "Pretty Weird" Path into Data'
author: ''
date: '2021-12-27'
slug: retrospective-my-slippery-slope-into-a-career-in-data
tags: ['career','learning']
output:
  blogdown::html_page:
    toc: false
---

Some time last year, I was interviewing for a lead analyst role at a really cool start-up[^1]. I didn't get the job, but during one of the rounds I got to speak with an executive who while scanning my LinkedIn during the call said something like:

[^1]: It feels weird to disclose interview experiences while you're employed, so I'm writing this during my transition period between companies. Poof, absolved of judgement.

*So you were doing customer success for a few years, and then became a data manager... that's pretty weird. How did that happen?*

After summarizing how it happened, he said: *You should be selling this story a lot harder.*

As I wrap up an 8+ year stint with my current company, my first since graduating college, I'd like to retrospect on some of the circumstances that helped me shift my career into data 4-5 years ago, and share a few anecdotes and insights from this period.

# "How to Learn Data Science" (Try googling it?)

I've probably read over a hundred *How I Learned \<insert data skill>* blog posts, each of which typically suggests a variety of online resources, including interactive coding modules, MOOCs, videos, and books. Some of the same resources are mentioned repeatedly, but more often than not, the resources recommended in these posts have very little overlap.

There's no doubt that some resources are higher quality than others, but I think the lack of overlap reveals that there are many perfectly viable resources and pathways to learning data skills. I'm not going to go into detail about the resources I used to learn R and SQL because:

1\) What worked for me might not work for someone else

2\) Lists of resources just isn't that interesting, but I'll chuck some of my favorites in the footnotes anyway[^2]

[^2]: Several [DataCamp](https://datacamp.com) courses, [Introduction to R for Data Science](https://learning.edx.org/course/course-v1:Microsoft+DAT204x+1T2016/home) (edX), [Sabermetrics 101: Introduction to Baseball Analytics](https://courses.edx.org/courses/BUx/SABR101x/2T2014/course/) (edX), and the process of writing the previous posts on [seth.cool](https://www.seth.cool/about/).

The short version is: I invested a lot of time outside of my job learning data transformation and visualization, and slowly integrated these skills into the workflows of relevant use cases *on the job*. Here's my GitHub contribution graph from my first year-ish of learning R, in 2017 and 2018, respectively.

![I started learning R in early 2017, but didn't embrace GitHub until later in the year.](files/github-contributions-2017-18.png)

# Baby's First CSV File

I joined [Discovery Education](discoveryeducation.com) in 2013 as a Customer & Tech Support rep, where I was responsible for answering calls and responding to emails and live chats from classroom teachers who needed assistance with the DE learning platform. Think lots of password resets.

Some background: I was *thrilled* to land this job[^3], having recently graduated with a B.S. in Kinesiology, an interest in working in sports, but no luck landing a full-time gig. I knew what data was in general, and have always loved baseball stats, but a career in data was absolutely not on my radar.

[^3]: Big shoutout to my friend from college, Ashley, who spent *maybe* six weeks in the same role before resigning to pursue a career in teaching

During my onboarding, one of my new colleagues introduced to the concept of "csv files," which customers (school districts) would provide import their school rosters into our platform, generating user accounts in bulk. I'd used Excel a couple of times, but hadn't heard of this kind of file before. My colleague explained that csv stood for *comma separated values*, and we moved onto the next training topic.

This is a boring anecdote on its own, but it feels good to occasionally acknowledge that some of the things and skills we use all day every day were completely unfamiliar, not that long ago. Despite a few bumpy patches related to leading zeroes and character encodings, my relationship with CSV files today has never been stronger.

*Lesson Learned #1:* Presented with an unfamiliar acronym, ask for the spelled out version.

# Building a Shadow CRM[^4]

[^4]: As in, not the officially-supported company CRM. Something I created on my own, like: `account_renewals_2015-03-24_SethG_final_v2.xlsx`

After almost a year on the Customer Support team, I was lucky enough to join the Account Services team (informally, The Renewals Team), where I was responsible for supporting and renewing portfolio of \~1,200 single-school customers. The more senior colleagues on the team managed similar books of business, but with fewer, larger accounts. Most of my days involved several hours of rapid-fire outbound phone calls, and juggling tracking grids we each maintained on our laptops, along with the actual CRM.

Every Monday, the team would receive another Excel file from automated email from a BI system[^5], which had several tabs worth of financial data. The main tab included summaries of each of my teammates' assigned customer subscriptions that were due for renewal, and their corresponding progress toward renewal goals. The other tabs were irrelevant to me (at the time \<*cue ominous music>*) -- they had the raw, granular data that powered the summary tab.

[^5]: Someday there will be a less corporate, soul-crushing name for these systems than Business Intelligence :(

Each team member would essentially maintain their own personal CRM. It involved a sacred ritual of VLOOKUP-ing the latest renewal status information from the weekly financials file, to their personal book of business file. Once our personal files were updated, we were left with our list of outstanding subscription renewals from which to work against.

*Lesson Learned #2*: There is a crazy amount of potential business value hidden away in deeply embedded, unarticulated workarounds that operational teams rely on. Unearthing and understanding these workarounds helps design data systems that people will... actually use! ðŸ™ƒ

For someone like me with 1,200 accounts, my biggest challenge was determining how to prioritize which ones to contact first. I typically would sort my list to prioritize by renewal due date, subscription value, and state, but it still felt pretty arbitrary.

I started to keep a list of things I wish I knew about my assigned schools prior to contacting them, including product usage information, customer tenure, and several other attributes. Fortunately, I sat directly next to a colleague who was the company's go-to *hey-can-you-pull-this-data-for-me-real-quick* guy. Every month or so, I would ask him to export product usage data, which I would then add to my personal file, helping me tailor my approach to each customer conversation.

*Lesson Learned #3:* Pick the brains of the incumbent go-to data people. Instead of positioning data requests as a transaction, ask for consultation, learn the caveats, and negotiate their recommendations.

Having personally reaped the benefits of incorporating traditionally-siloed data into my personal sheet, I started to help my teammates with the same. Over time, I started to accumulate and compile data from even more sources, such as customer support cases, customer tenure, indicators for customers that had enabled product integrations, and more. In hindsight, I had helped create something primitively resembling what Benn Stancil envisions with his [Yelp for the Enterprise](https://benn.substack.com/p/the-future-of-operational-analytics) analogy. The team started to rely on my compilation and distribution of the *enhanced* customer metrics every week. It helped us all be better at our jobs.

*Lesson Learned #4:* Find small ways that existing workflows can be noticeably improved via data visibility, and then prove it. As beneficial as the improved workflow may be, proving the value-add is *the best* way to earn yourself more opportunities to work on data-related projects.

*Lesson Learned #5:* Create feedback loops. Pass thank you notes you receive onto the folks who supplied the raw data for you. They might be satisfied to know that their work has helped make an impact... and they will be less annoyed the next time you request a data pull. Also, it's important that your manager gets to see the evidence of return on investment.

# Automating the Boring Stuff

It's worth acknowledging that spending several hours a week munging and mashing several spreadsheets together was very much not what I was being paid to do. Throughout this experience, it was extremely important to earn and retain the approval of my management to continue working on this, however, I was ultimately goaled on (and mostly compensated by) how well I performed against my renewal targets. I needed to find a way to support my Shadow BI efforts, and still have enough time to call my assigned schools -- my, uh... actual job.

I was starting to learn R outside of work, on the recommendation of a friend whom I complained to about my Excel files constantly freezing my computer. After a while, I figured out how to translate my step-by-step data munging process into an R script, which could import each of the component Excel files as ingredients, and output a tasty, ready-to-distribute data meal for my teammates. The upfront investment of trial-and-erroring my way to a working R script ended up saving me hours of repetitive, and error-prone filtering, VLOOKUP-ing, and aggregation steps. Automating the boring stuff[^6] allowed me to maintain my impact, while unlocking capacity elsewhere.

[^6]: I've borrowed this phrase from the popular programming book, [Automate the Boring Stuff with Python](https://automatetheboringstuff.com/), which teaches readers to complete tedious professional tasks using Python.

*Lesson Learned #6:* Take stock of repetitive tasks, and seek out ways to automate them. This creates new blocks of time to work on high impact problems. Don't worry about automating yourself out of a job[^7]. Think of it as automating your way into a more impactful job.

[^7]: Sweeping generalizations about automation is an American tradition. The New York Times has dozens of articles over the past 70 years on the topic, like [Fears About Automation Are Overshadowing Benefits to U.S.](https://nyti.ms/3mZbQvw)\_ and [Button Pushers Are Worried, Too](https://nyti.ms/3ePtHk2), printed in 1961.

Lesson Learned #7: One of the side effects of building Shadow BI tools is the development of relatively subject matter expertise on several domains within the business, which helps unlock future opportunities. \<reword this>

# My First Real Data Job

By the end of 2017, I'd negotiated a reduced account load, in order to dedicate \~40% of my time toward data analysis work in support of our renewal team. This was a big win for me, but by this point I wanted to go all-in. Thanks to a recent corporate re-org, and the advocacy of my manager, I had an opportunity to apply for an open Salesforce Data Manager role, on the company's Business Systems team.

I don't remember much from the interview process, other than the hiring manager (and soon-to-be manager) telling me that the role would heavily rely on SQL for data migrations, analysis, and more. I didn't know *any* SQL, but had collected a few queries from my go-to *pull-some-data-real-quick* friends over the years, which helped me build basic familiarity with the syntax. I shared that I was comfortable in R, and was learning SQL, but still considered myself to be a beginner.

*Lesson Learned #8:* New skills (e.g., SQL) can be learned relatively easier than subject matter knowledge. Sometimes you can borrow against your subject matter knowledge to earn opportunities that you're completely unqualified for on paper [ðŸ˜Ž](https://emojipedia.org/smiling-face-with-sunglasses/)

For the first couple months, I leaned on my R experience to compensate for my lack of SQL knowledge. Rather than querying the database directly, I relied on exporting CSV files from the Salesforce front-end, importing them into RStudio, and doing whatever needed to be done. After a while, I recognized this workflow to include several minutes of manual, repetitive work, which nudged me into learning:

1.  How to translate my point-and-click queries using Salesforce's report builder into SQL. It helped that I was comfortable with the concept of [joins](https://www.w3schools.com/sql/sql_join.asp) from using the [dplyr](https://dplyr.tidyverse.org/) R package.
2.  How to send queries to the database from RStudio, and receive the results as a data table

Once again, automating the boring stuff helped me unlock blocks of time to learn new skills, and work on impactful projects.

# Strategic Laziness and 50X Productivity Gains 

About nine months into my Data Manager role, I was asked to temporarily support the product analytics team work through a massive backlog of customer-facing usage report requests, during a particularly busy renewal season. The process of generating usage reports at the time involved generating data visualizations in Microsoft PowerBI, which pulled data from a linked Excel file, which itself was refreshed by several SQL Server stored procedures, then copying the visualizations into slides and/or documents, then sending the output to the district's customer success rep. The start-to-finish process of generating a single report felt extremely manual, and took about an hour.

After two days and only a handful of reports to show for, I floated the idea of exploring more automated solutions to the primary stakeholder, who would eventually become my manager a year later. Before investing any time or effort into a less manual process, I wanted to make sure there was an openness to change the current process, and to gauge the tolerance for a potentially less-polished final output, given that the backlog was several hundred reports long. She was extremely open to iterating on the current process, which she admitted was a stopgap solution prior to investing in a true Business Intelligence tool[^8].

[^8]: After a lengthy selection process, we signed on with Looker

As luck would have it, I had recently skimmed a blog post from the Urban Institute's data team about creating parameterized fact sheets with R Markdown[^9]. The blog post stepped through an example of generating a series of reports on car crash data, one for each State. They were able to do so by creating a parameter, or filter value, which could be updated to *Maryland* for example, and generate a report with Maryland-only data.

[^9]: [Iterated fact sheets with R Markdown](https://urban-institute.medium.com/iterated-fact-sheets-with-r-markdown-d685eb4eafce), by Aaron Williams of The Urban Institute

This was *exactly* the kind of solution I was looking for to automate my way through the backlog of report requests. It took me a few days to get everything working the way I had hoped, but I ended up creating an RMarkdown file that:

-   Took parameter values for School District ID, Start and End Dates, and Products

-   Piped the parameter values into a parameterized SQL query

-   Sent the query to SQL Server, and returned the results as a data frame

-   Used the data frame to generate several graphs using R's `ggplot2`

-   Used the data frame enrich the report's written narrative with inline metrics (i.e., Between `<start date>` and `<end date>`, `<Name of School District>` had `<number of students>` Active Students.)

I went back and forth with the stakeholder(s) to agree on a standardized format, and made clear that this would be *the* format for all reports. In exchange, I committed to being able to run 10 reports a day, rather than 3.

So, why is this section called *Strategic Laziness?*

This 3X productivity commitment was met with excitement, but was actually a pretty conservative estimate. The old process took me about an hour, while the new process could generate a report in 30 seconds âš¡. As I started using the process, I ended up delivering between 30 and 50 reports a day: a 10-15X+ productivity gain. If I dedicated a whole day generating reports, I think I could have easily pumped out 150 reports: a 50X gain, but I never did this, and I think for good reason.

Sometimes doing *less* of your assigned work can lead to an increased impact. This seems counterintuitive, and could be mistaken for poor performance if managed incorrectly. I recently heard Matt Greenberg[^10], CTO of [Reforge](https://www.reforge.com/), refer to this as "modulating your output."

[^10]: I'm also quite fond of Matt's Monkeys vs. Rocks analogy, which he describes in this [Twitter thread](https://twitter.com/matt_muffin/status/1360248407659479044)

I think the trick is finding a balance where your output meets (or exceeds) the status quo (i.e., stakeholders are happy), which unlocks additional time to dedicate to other projects, learning, etc.

# Recap

It's been tough to distill this years-long transition into a few hundred words. I've also found it tricky to articulate the major takeaways from each anecdote -- each of the patterns emerged implicitly, but weren't things I necessarily talked about with colleagues, or bored my friends with.

Looking ahead to hopefully several more decades of working (in data or elsewhere), one thing I know for sure is that the world is changing exponentially[^11], and we'll all have to create the time to keep up with it.

[^11]: Packy McCormick's [Compounding Crazy](https://www.notboring.co/p/compounding-crazy) essay was a real eye-opener on the exponential nature of innovation, and the concept of the [Die Progress Unit](https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html).
